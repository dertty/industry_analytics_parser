{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Иcточники данных\n",
    "\n",
    "- Внешние источники\n",
    "  - b1: `b1_analytics.pkl`\n",
    "  - kamaflow: `kamaflow_researches.pkl`\n",
    "- Внутренние источники\n",
    "  - Отраслевой хаб: данные выложены в sberdisk\n",
    "\n",
    "## Формат данных \n",
    "\n",
    "Результат представляет собой массив словарей, где каждый словарь содержит информацию об одном аналитическом материале. Каждый словарь имеет следующие ключи:\n",
    "\n",
    "### Пример элемента списка `analytics`:\n",
    "```json\n",
    "{\n",
    "    \"link\": \"/news/12345\",\n",
    "    \"date\": \"2023-10-15\",\n",
    "    \"name\": \"Анализ рынка ИТ в 2023 году\",\n",
    "    \"name_en\": \"it-market-analysis-2023\",\n",
    "    \"tags\": [\"ИТ\", \"аналитика\", \"2023\"],\n",
    "    \"pdf_links\": [\n",
    "        \"https://b1.ru/local/assets/surveys/it-market-analysis-2023.pdf\"\n",
    "    ],\n",
    "    \"text\": \"В 2023 году рынок ИТ показал рост на 14%...\",\n",
    "    \"pdfs\": [\n",
    "        {\n",
    "            \"name\": \"it-market-analysis-2023.pdf\",\n",
    "            \"content\": \"binary_pdf_content_here\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "Мы хотим получить такой, при этом часть полей можент быть пустой:\n",
    "```json\n",
    "{\n",
    "    \"link\": \"/news/12345\",\n",
    "    \"date\": \"2023-10-15\",\n",
    "    \"name\": \"Анализ рынка ИТ в 2023 году\",\n",
    "    \"name_en\": \"it-market-analysis-2023\",\n",
    "    \"tags\": [\"ИТ\", \"аналитика\", \"2023\"],\n",
    "    \"sber_tags\": [\"Теги\", \"из\", \"сбера\", \"отрасль\", \"из\", \"kksb_clinet_profile\"],\n",
    "    \"pdf_links\": [\n",
    "        \"https://b1.ru/local/assets/surveys/it-market-analysis-2023.pdf\"\n",
    "    ],\n",
    "    \"text\": \"В 2023 году рынок ИТ показал рост на 14%...\",\n",
    "    \"summary\": \"Саммари на основе pdfs\",\n",
    "    \"pdfs\": [\n",
    "        {\n",
    "            \"name\": \"it-market-analysis-2023.pdf\",\n",
    "            \"content\": \"binary_pdf_content_here\",\n",
    "            \"full_text\": \"Полный текст и pdf, то что получилось вытащить из content\",\n",
    "            \"summary\": \"Саммари из full_text\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Для дальнейнего анализа нужно только следующие поля: date, sber_tags, summary, pdfs['full_text'], pdfs['summary']:\n",
    "```json\n",
    "{\n",
    "    \"date\": \"2023-10-15\",\n",
    "    \"sber_tags\": [\"Теги\", \"из\", \"сбера\", \"отрасль\", \"из\", \"kksb_clinet_profile\"],\n",
    "    \"summary\": \"Саммари\",\n",
    "    \"pdfs\": [\n",
    "        {\n",
    "            \"full_text\": \"Полный текст и pdf, то что получилось вытащить из content\",\n",
    "            \"summary\": \"Саммари из full_text\",\n",
    "        },\n",
    "        {\n",
    "            \"full_text\": \"Полный текст и pdf, то что получилось вытащить из content\",\n",
    "            \"summary\": \"Саммари из full_text\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('b1_analytics.pkl', 'rb') as f:\n",
    "    b1_analytics = pickle.load(f)\n",
    "\n",
    "with open('kamaflow_researches.pkl', 'rb') as f:\n",
    "    kamaflow_researches = pickle.load(f)\n",
    "\n",
    "# with open('/Users/22926900/Desktop/consult_plan_v3/industry_analytics_parser/notebooks/industry_hub_analytics_update.pkl', 'rb') as f:\n",
    "#     industry_hub_analytics = pickle.load(f)\n",
    "\n",
    "docs = b1_analytics + kamaflow_researches #+ industry_hub_analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_json_blocks(text):\n",
    "    \"\"\"\n",
    "    Извлекает содержимое всех блоков ```json ... ``` из текста.\n",
    "    \n",
    "    Аргументы:\n",
    "        text (str): Исходный текст, содержащий блоки с JSON.\n",
    "        \n",
    "    Возвращает:\n",
    "        str: Содержимое всех JSON-блоков, объединенное через пробел.\n",
    "             Если блоков не найдено, возвращает пустую строку.\n",
    "    \"\"\"\n",
    "    pattern = r'```json(.*?)```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    \n",
    "    # Объединяем все найденные блоки через пробел и убираем лишние пробелы\n",
    "    result = ' '.join(match.strip() for match in matches)\n",
    "    return result\n",
    "\n",
    "\n",
    "def clean_json(broken_json: str) -> str:\n",
    "    \"\"\"\n",
    "    1) Убирает висячие запятые перед ']' или '}'.\n",
    "    2) Экранирует внутренние кавычки внутри строковых значений.\n",
    "    Возвращает исправленный JSON в виде строки.\n",
    "    \"\"\"\n",
    "    # 1) Удаляем запятые перед ] или }\n",
    "    no_trailing = re.sub(r',\\s*(?=[}\\]])', '', broken_json)\n",
    "\n",
    "    # 2) Функция для экранирования внутренних кавычек\n",
    "    def escape_inner_quotes(s: str) -> str:\n",
    "        res = []\n",
    "        inside = False\n",
    "        i = 0\n",
    "        while i < len(s):\n",
    "            c = s[i]\n",
    "            if c == '\"' and (i == 0 or s[i-1] != '\\\\'):\n",
    "                if not inside:\n",
    "                    inside = True\n",
    "                    res.append(c)\n",
    "                else:\n",
    "                    # lookahead: если за кавычкой идёт :, , , ] или }, считаем её закрывающей\n",
    "                    j = i + 1\n",
    "                    while j < len(s) and s[j].isspace():\n",
    "                        j += 1\n",
    "                    if j < len(s) and s[j] in [':', ',', ']', '}']:\n",
    "                        inside = False\n",
    "                        res.append(c)\n",
    "                    else:\n",
    "                        # внутренняя кавычка — экранируем\n",
    "                        res.append('\\\\\"')\n",
    "                i += 1\n",
    "            elif c == '\\\\' and inside:\n",
    "                # сохраняем существующие escape-последовательности без изменения\n",
    "                if i + 1 < len(s):\n",
    "                    res.append(c)\n",
    "                    res.append(s[i+1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    res.append(c)\n",
    "                    i += 1\n",
    "            else:\n",
    "                res.append(c)\n",
    "                i += 1\n",
    "        return ''.join(res)\n",
    "\n",
    "    cleaned = escape_inner_quotes(no_trailing)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def fix_json(json_str: str) -> str:\n",
    "    try:\n",
    "        json.loads(json_str)\n",
    "    except:\n",
    "        try:\n",
    "            json.loads(extract_json_blocks(json_str))\n",
    "        except:\n",
    "            try:\n",
    "                json_str = clean_json(json_str)\n",
    "                json.loads(json_str)\n",
    "            except:\n",
    "                import dirtyjson\n",
    "                print(json_str)\n",
    "                json_str = json.dumps(dict(dirtyjson.loads(json_str)), ensure_ascii=False, indent=4)\n",
    "    return json_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавляем саммари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1-nano\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Ты — стратегический консультант. На основе отраслевых данных, новостей и исследований тебе нужно выявить 1–3 ключевых тренда, подтверждённых фактами, и представить их в виде валидного JSON по заданной схеме.\n",
    "\n",
    "## Твоя задача:\n",
    "1. Найди до трёх трендов из текста.\n",
    "2. Каждый тренд должен быть явно выражен, подтверждён конкретным фактом и проанализирован.\n",
    "3. Анализ включает причины, последствия, влияние на рынок и прогноз развития (3–7 предложений).\n",
    "4. Пиши только на русском языке.\n",
    "\n",
    "## Формат вывода (JSON):\n",
    "```json\n",
    "[\n",
    "  {{\n",
    "    \"тренд\": \"Краткое утверждение, отражающее суть тренда (например: \\\\\"рост спроса на экологичные товары\\\\\")\",\n",
    "    \"факт\": \"Цифры, статистика или цитата из текста, подтверждающая тренд (например: \\\\\"73% потребителей готовы платить больше за экологичную упаковку\\\\\")\",\n",
    "    \"анализ\": \"Связный аналитический текст из 3–7 предложений, раскрывающий причины, влияние, последствия и прогноз тренда\"\n",
    "  }}\n",
    "]\n",
    "```\n",
    "\n",
    "## Требования к JSON:\n",
    "- Используй только двойные кавычки (\")\n",
    "- После каждого поля ставь запятую, кроме последнего\n",
    "- Не используй комментарии\n",
    "- Все кавычки внутри строк экранируй: \\\\\" \n",
    "- Экранируй спецсимволы: \\\\\\\\ — обратная косая, \\\\n — перенос строки, \\\\t — табуляция\n",
    "- При отсутствии трендов верни: `[]`\n",
    "- Все кавычки внутри строк обязательно экранируй: \\\\\" — иначе JSON будет невалидным\n",
    "- JSON должен быть полностью валидным\n",
    "\n",
    "## Входные данные:\n",
    "- Название: {name}\n",
    "- Теги: {tags} \n",
    "- Краткое содержание: {additional_text}\n",
    "- Текст документа:\n",
    "```\n",
    "{text_document}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "messages = ChatPromptTemplate.from_messages([\n",
    "    ('system', prompt),\n",
    "    ('human', \"Выведи итоговый JSON\"),\n",
    "])\n",
    "\n",
    "\n",
    "chain = (\n",
    "    messages\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "Ты — аналитик и JSON-корректор. На входе — повреждённый JSON со списком трендов, содержащий синтаксические ошибки (неэкранированные кавычки, лишние запятые, некорректные структуры, обрывки текста и др.).\n",
    "\n",
    "Твоя задача:\n",
    "1. Постараться восстановить корректный JSON по смыслу и структуре.\n",
    "2. Привести его к строго заданному формату (см. ниже).\n",
    "3. Если какие-то элементы повреждены — реконструируй их по контексту.\n",
    "4. Если исходные данные неполные или в них **нет осмысленных трендов**, верни пустой список: `[]`.\n",
    "\n",
    "## Формат вывода (JSON):\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"тренд\": \"Краткое утверждение, отражающее суть тренда (например: \\\"рост спроса на экологичные товары\\\")\",\n",
    "    \"факт\": \"Цифры, статистика или цитата из текста, подтверждающая тренд (например: \\\"73% потребителей готовы платить больше за экологичную упаковку\\\")\",\n",
    "    \"анализ\": \"Связный аналитический текст из 3–7 предложений, раскрывающий причины, влияние, последствия и прогноз тренда\"\n",
    "  }\n",
    "]\n",
    "'''\n",
    "\n",
    "user_prompt = '''\n",
    "Проанализируй и исправь следующий повреждённый JSON:\n",
    "\n",
    "{json}\n",
    "'''\n",
    "fix_messages = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_prompt),\n",
    "    ('human', user_prompt),\n",
    "])\n",
    "\n",
    "\n",
    "fix_chain = (\n",
    "    fix_messages\n",
    "    | llm \n",
    "    | JsonOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 95/99 [08:14<00:31,  7.81s/it]invalid pdf header: b'RIFF\\x8c'\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing PDF Супертренд на здоровое питание — ключевая тенденция на рынке хлебобулочных и кондитерских изделий в России: Stream has ended unexpectedly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [08:25<00:00,  5.10s/it]\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for doc in tqdm(docs):\n",
    "    name = doc['name']\n",
    "    tags = doc['tags']\n",
    "    additional_text = doc['text']\n",
    "    \n",
    "    text_documents = []\n",
    "    \n",
    "    for pdf in doc['pdfs']:\n",
    "        pdf_name = pdf['name']\n",
    "\n",
    "        pdf_content = pdf.get('content', None)\n",
    "        full_text = pdf.get('full_text', None)\n",
    "        if full_text is None or len(full_text) > 0:\n",
    "            counter += 1\n",
    "        if full_text is None and pdf_content is not None:\n",
    "            try:\n",
    "                with tempfile.NamedTemporaryFile() as temp_file:\n",
    "                    temp_file.write(pdf_content)\n",
    "                    temp_file_path = temp_file.name\n",
    "                    loader = PyPDFLoader(temp_file_path)\n",
    "                    pdf_data = loader.load()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing PDF {pdf_name}: {e}\")\n",
    "                pdf_data = None\n",
    "            if pdf_data:\n",
    "                full_text = \"\\n\".join([doc.page_content for doc in pdf_data])\n",
    "                pdf['full_text'] = full_text\n",
    "        \n",
    "        if full_text:\n",
    "            if additional_text is None or len(additional_text) == 0:\n",
    "                additional_text = 'нет краткого описание'\n",
    "            text_document = f'**Документ с наименованием {pdf_name}, его содержимое:** {full_text}'\n",
    "            text_documents.append(text_document)\n",
    "    if len(text_documents) == 0:\n",
    "        doc['summary'] = ''\n",
    "    else:\n",
    "        text_document = \"\\n\".join(text_documents)\n",
    "        trends = chain.invoke(\n",
    "                {\n",
    "                    'name': name,\n",
    "                    'tags': \", \".join(tags), \n",
    "                    'additional_text': additional_text,\n",
    "                    'text_document': text_document[:1_000 - 3] + '...',\n",
    "                }\n",
    "            )\n",
    "        try:\n",
    "            trends = json.loads(fix_json(trends))\n",
    "        except:\n",
    "            trends = fix_chain.invoke({'json': trends})\n",
    "        doc['summary'] = trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавляем теги\n",
    "Теги из kksb_client_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [01:27<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Данные\n",
    "data = {\n",
    "    \"date\": \"2023-10-15\",\n",
    "    \"summary\": \"Саммари\",\n",
    "    \"pdfs\": [\n",
    "        {\n",
    "            \"full_text\": \"Полный текст и pdf, то что получилось вытащить из content\",\n",
    "        },\n",
    "        {\n",
    "            \"full_text\": \"Полный текст и pdf, то что получилось вытащить из content\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# Возможные теги\n",
    "tags_list = [\n",
    "    'Энергетика', 'Лизинг', 'Нефтегазовая промышленность', 'Пищевая промышленность',\n",
    "    'Финансы', 'Органы государственного и муниципального управления', 'Материалы', 'Промышленность',\n",
    "    'Телекоммуникации', 'Телекоммуникации и медиа', 'ИТ-технологии', 'ЖКХ', 'Услуги',\n",
    "    'Электроэнергетика и ЖКХ', 'Строительные подрядчики', 'Сельское хозяйство', 'Товары первой необходимости',\n",
    "    'Энергоносители', 'Товары выборочного спроса', 'Операции с недвижимым имуществом', 'Государство',\n",
    "    'Недвижимость', 'Фармацевтика и здравоохранение', 'Химическая промышленность', 'Розничная торговля товарами первой необходимости',\n",
    "    'Машиностроение', 'Металлургическая и горнодобывающая промышленность', 'Производство строительных материалов',\n",
    "    'Розничная торговля товарами выборочного спроса', 'Финансовая деятельность', 'Легкая промышленность',\n",
    "    'Транспорт и логистика', 'Производство потребительских товаров', 'Лесная и деревообрабатывающая и целлюлозно-бумажная промышленность', 'Туризм'\n",
    "]\n",
    "tags_list = [tag.lower() for tag in tags_list]\n",
    "\n",
    "for data in tqdm(docs):\n",
    "    text_sources = [\n",
    "        'Отраслевые теги из документа: ',\n",
    "        ', '.join([x for x in data[\"tags\"] if x != 'Прочее']), \n",
    "        '\\n Текст трендов: ', str(data[\"summary\"]), \n",
    "    ]\n",
    "    context_text = \"\\n\".join(filter(None, text_sources))[:40_000]\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\", \"tags\"],\n",
    "        template=(\n",
    "            \"\"\"Определи, какие из следующих тегов отрасли наиболее подходят для данного текста с отраслевыми трендами:\n",
    "            ## Теги отрасли: \n",
    "            {tags}. \n",
    "            ## Тренды:\n",
    "            {text}\n",
    "            ## Формат ответа:\n",
    "            Отвечай только списком релевантных трендам тегов через запятую, без пояснений. Пиши только те теги, которые релевантны трендам и есть в списке теги отрасли:{tags}. Не выдумывай другие теги и не изменя  формулировку этих тегов. Не пиши ничего кроме тегов. Если релевантных тегов нет не пиши ничего.\"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(prompt.format(text=context_text, tags=\", \".join(tags_list))).content.strip()\n",
    "    tags = [tag.strip() for tag in response.split(\",\") if tag.strip().lower() in tags_list]\n",
    "    if len(tags) == 0:\n",
    "        data[\"sber_tags\"] = None\n",
    "    else:\n",
    "        data[\"sber_tags\"] = ', '.join(list(set(tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сохраняемся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs.pkl', 'wb') as f:\n",
    "    pickle.dump(docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_str):\n",
    "    # Пробуем разные форматы дат\n",
    "    for fmt in ('%d.%m.%Y', '%Y-%m-%d', '%d-%m-%Y', '%Y.%m.%d', '%y-%m-%d', '%d/%m/%Y'):\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt, errors='raise')\n",
    "        except:\n",
    "            continue\n",
    "    return pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(docs).rename(columns={'date': 'date_sum'})[['date_sum', 'sber_tags', 'summary']]\n",
    "df['date_sum'] = df['date_sum'].apply(parse_date).dt.strftime('%d.%m.%Y')\n",
    "df['sber_tags'] = df['sber_tags'].str.replace('\\s*,\\s*', ',', regex=True).str.split(',')\n",
    "df = df.explode('sber_tags')\n",
    "df['sber_tags'] = df['sber_tags'].str.strip()\n",
    "df = df.reset_index().rename(columns={'index': 'row_id'})[['date_sum', 'sber_tags', 'summary', 'row_id']]\n",
    "df = df.dropna(subset=['date_sum'], how='any')\n",
    "df.to_excel('trends_with_dates.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
