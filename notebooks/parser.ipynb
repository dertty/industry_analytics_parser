{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Внешние отраслевые источники"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def get_soup(link):\n",
    "    HEADERS = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    soup = None\n",
    "    response = requests.get(link, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    else:\n",
    "        print(f\"Ошибка запроса: {response.status_code}\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "def convert_to_text(soup):\n",
    "    html_converter = html2text.HTML2Text()\n",
    "    html_converter.ignore_links = True\n",
    "    html_converter.body_width = 0\n",
    "    markdown_text = html_converter.handle(str(soup))\n",
    "    return markdown_text\n",
    "\n",
    "\n",
    "def convert_date(date_str):\n",
    "    months = {\n",
    "        'январь': 1, 'февраль': 2, 'март': 3, 'апрель': 4,\n",
    "        'май': 5, 'июнь': 6, 'июль': 7, 'август': 8,\n",
    "        'сентябрь': 9, 'октябрь': 10, 'ноябрь': 11, 'декабрь': 12\n",
    "    }\n",
    "    month_str, year_str = date_str.lower().split()\n",
    "    month = months.get(month_str)\n",
    "    year = int(year_str)\n",
    "\n",
    "    return datetime(year, month, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результатом пасинга с каждого сайта будет аля jsonlines формата:\n",
    "\n",
    "## Формат данных \n",
    "\n",
    "Результат представляет собой массив словарей, где каждый словарь содержит информацию об одном аналитическом материале. Каждый словарь имеет следующие ключи:\n",
    "\n",
    "### Ключи словаря:\n",
    "1. **`link`** (`str`):\n",
    "   - Ссылка на страницу с аналитическим материалом. Может быть как полная, так и относительная\n",
    "   - Пример: `\"/news/12345\"`.\n",
    "\n",
    "2. **`date`** (`str`):\n",
    "   - Дата публикации материала.\n",
    "   - Пример: `\"2023-10-15\"`.\n",
    "\n",
    "3. **`name`** (`str`):\n",
    "   - Название материала на русском языке.\n",
    "   - Пример: `\"Анализ рынка ИТ в 2023 году\"`.\n",
    "\n",
    "4. **`name_en`** (`str`):\n",
    "   - Название материала на английском языке, извлеченное из ссылки.\n",
    "   - Пример: `\"it-market-analysis-2023\"`.\n",
    "\n",
    "5. **`tags`** (`list` of `str`):\n",
    "   - Список тегов, связанных с материалом.\n",
    "   - Пример: `[\"ИТ\", \"аналитика\", \"2023\"]`.\n",
    "\n",
    "6. **`pdf_links`** (`list` of `str`):\n",
    "   - Список ссылок на PDF-файлы, связанные с материалом.\n",
    "   - Пример: `[\"https://b1.ru/local/assets/surveys/it-market-analysis-2023.pdf\"]`.\n",
    "\n",
    "7. **`text`** (`str`):\n",
    "   - Основной текст материала из статьи на сайте (в pdf больше информации), очищенный от лишних элементов (контакты, реклама, скрипты и т.д.).\n",
    "   - Пример: `\"В 2023 году рынок ИТ показал рост на 14%...\"`.\n",
    "\n",
    "8. **`pdfs`** (`list` of `dict`):\n",
    "   - Список словарей, содержащих информацию о скачанных PDF-файлах. Каждый словарь содержит:\n",
    "     - **`name`** (`str`): Название PDF-файла.\n",
    "       - Пример: `\"it-market-analysis-2023.pdf\"`.\n",
    "     - **`content`** (`bytes`): Бинарное содержимое PDF-файла.\n",
    "\n",
    "---\n",
    "\n",
    "### Пример элемента списка `analytics`:\n",
    "```json\n",
    "{\n",
    "    \"link\": \"/news/12345\",\n",
    "    \"date\": \"2023-10-15\",\n",
    "    \"name\": \"Анализ рынка ИТ в 2023 году\",\n",
    "    \"name_en\": \"it-market-analysis-2023\",\n",
    "    \"tags\": [\"ИТ\", \"аналитика\", \"2023\"],\n",
    "    \"pdf_links\": [\n",
    "        \"https://b1.ru/local/assets/surveys/it-market-analysis-2023.pdf\"\n",
    "    ],\n",
    "    \"text\": \"В 2023 году рынок ИТ показал рост на 14%...\",\n",
    "    \"pdfs\": [\n",
    "        {\n",
    "            \"name\": \"it-market-analysis-2023.pdf\",\n",
    "            \"content\": \"binary_pdf_content_here\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b1-surveys\n",
    "Парсим данные со страницы https://b1.ru/b1-surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [50:51<00:00, 435.90s/it]   \n"
     ]
    }
   ],
   "source": [
    "URL = 'https://b1.ru/'\n",
    "SURVEYS_URL = URL + 'b1-surveys/'\n",
    "\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "soup = get_soup(SURVEYS_URL)\n",
    "pagination = soup.find('div', class_='pagination')\n",
    "page_links = [a['href'] for a in pagination.find_all('a') if '?PAGEN_2=' in a['href']]\n",
    "last_page = max([int(a.split('?PAGEN_2=')[-1]) for a in page_links])\n",
    "analytics = []\n",
    "for i in tqdm(range(1, last_page + 1)):\n",
    "    soup = get_soup(SURVEYS_URL + '?PAGEN_2=' + str(i))\n",
    "    time.sleep(1)\n",
    "    for block in soup.find_all('div', class_=re.compile(r\"^news-block__gridItem triangleWrap--hover\")):\n",
    "        if block:\n",
    "            link = block['src-href']\n",
    "            if not 'services' in link:\n",
    "                analytics_block = {}\n",
    "                analytics_block['link'] = link\n",
    "                analytics_block['date'] = datetime.strptime(block.find('p', class_='news-block__date').text, '%d.%m.%Y')\n",
    "                analytics_block['name'] = block.find('h2', class_='news-block__title').text\n",
    "                analytics_block['name_en'] = link.strip().strip('/').split('/')[-1]\n",
    "                \n",
    "                time.sleep(1)\n",
    "                analytics_soup = get_soup(URL + link)\n",
    "                tags_block = analytics_soup.find('div', class_=re.compile(r\"tezis-list-block__tags\"))\n",
    "                if tags_block:\n",
    "                    analytics_block['tags'] = [a.text for a in tags_block.find_all('a', class_=re.compile(r\"tag\"))]\n",
    "                else:\n",
    "                    analytics_block['tags'] = []\n",
    "                \n",
    "                analytics_block['pdf_links'] = [f'{URL}{a['href']}' for a in analytics_soup.find_all('a', href=True) if a['href'].endswith('.pdf')]\n",
    "                analytics_block['pdf_links'].append(f'{URL}local/assets/surveys/{analytics_block['name_en']}.pdf')\n",
    "\n",
    "                main_text = analytics_soup.find('main')\n",
    "                for el in main_text.find_all('section', attrs={\"data-block-name\": \"Контакты\"}):\n",
    "                    el.extract()\n",
    "                for el in main_text.find_all('section', attrs={\"data-block-name\": \"СКАЧАТЬ ПОЛНУЮ ВЕРСИЮ\"}):\n",
    "                    el.extract()\n",
    "                for el in main_text.find_all('div', attrs={\"class\": \"page-cover\"}):\n",
    "                    el.extract() \n",
    "                for el in main_text.find_all('script'):\n",
    "                    el.extract()\n",
    "                for el in main_text.find_all('style'):\n",
    "                    el.extract()\n",
    "                if tags_block:\n",
    "                    for el in tags_block:\n",
    "                        el.extract()\n",
    "                analytics_block['text'] = convert_to_text(main_text)\n",
    "                \n",
    "                pdfs_content = []\n",
    "                pdfs_names = []\n",
    "                for url in analytics_block['pdf_links']:\n",
    "                    for i in range(10):\n",
    "                        try:\n",
    "                            response = session.get(url, stream=True)\n",
    "                            if response.status_code == 200:\n",
    "                                pdf_name = url.strip().strip('/').split('/')[-1]\n",
    "                                pdf_content = response.content\n",
    "                                if pdf_name not in pdfs_names:\n",
    "                                    pdfs_names.append(pdf_name)\n",
    "                                    pdfs_content.append({\"name\": pdf_name, \"content\": pdf_content})\n",
    "                                break\n",
    "                        except:\n",
    "                            time.sleep(3)\n",
    "                        \n",
    "                analytics_block['pdfs'] = pdfs_content\n",
    "                analytics.append(analytics_block)\n",
    "\n",
    "with open('b1_analytics.pkl', 'wb') as f:\n",
    "    pickle.dump(analytics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kamaflow\n",
    "Парсим данные с сайта https://kamaflow.com/ru/blog/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://strategy.ru'\n",
    "\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "url = URL + \"/api/v1/research/research\"\n",
    "params = {\"limit\": 10, \"offset\": 0}\n",
    "response = session.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(f\"Ошибка: {response.status_code}\")\n",
    "    print(response.text)\n",
    "\n",
    "researches_links = []\n",
    "k = 10\n",
    "for i in range(0, data['count'] + 1, k):\n",
    "    params = {\"limit\": k, \"offset\": i}\n",
    "    response = session.get(url, params=params)\n",
    "    data = response.json()\n",
    "    researches_links += data['researches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [11:20<00:00, 17.45s/it]\n"
     ]
    }
   ],
   "source": [
    "researches = []\n",
    "for research_block in tqdm(researches_links):\n",
    "    link = URL + research_block['absolute_url']\n",
    "    date = convert_date(research_block['published_at'])\n",
    "    tags = [ind['title'] for ind in research_block['industries']]\n",
    "    name = research_block['title']\n",
    "    text_preview = research_block['preview']\n",
    "    \n",
    "    info = {'link': link, 'date': date, 'name': name, 'text_preview': text_preview, 'tags': tags}\n",
    "    time.sleep(1)\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            soup = get_soup(info['link'])\n",
    "            publication_div = soup.find('div', class_='publication__text tiny')\n",
    "            pdf_links = []\n",
    "            for btn in publication_div.find_all('a', class_='btn-download'):\n",
    "                pdf_links.append(URL + btn.extract()['href'].replace('../', '/'))\n",
    "            for el in publication_div.find_all('script'):\n",
    "                    el.extract()\n",
    "            for el in publication_div.find_all('style'):\n",
    "                el.extract()\n",
    "            info['text'] = convert_to_text(publication_div)\n",
    "            info['pdf_links'] = list(set(pdf_links))\n",
    "            pdfs_content = []\n",
    "            for pdf_link in info['pdf_links']:\n",
    "                response = session.get(pdf_link, stream=True)\n",
    "                if response.status_code == 200:\n",
    "                    pdf_name = url.strip().strip('/').split('/')[-1]\n",
    "                    pdf_content = response.content\n",
    "                    pdfs_content.append({\"name\": info['name'], \"content\": pdf_content})\n",
    "            info['pdfs'] = pdfs_content\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "    \n",
    "    researches.append(info)\n",
    "with open('kamaflow_researches.pkl', 'wb') as f:\n",
    "    pickle.dump(researches, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
