{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Внешние отраслевые источники"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def get_soup(link):\n",
    "    HEADERS = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    soup = None\n",
    "    response = requests.get(link, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    else:\n",
    "        print(f\"Ошибка запроса: {response.status_code}\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "def convert_to_text(soup):\n",
    "    html_converter = html2text.HTML2Text()\n",
    "    html_converter.ignore_links = True\n",
    "    html_converter.body_width = 0\n",
    "    markdown_text = html_converter.handle(str(soup))\n",
    "    return markdown_text\n",
    "\n",
    "\n",
    "def convert_date(date_str):\n",
    "    months = {\n",
    "        'январь': 1, 'февраль': 2, 'март': 3, 'апрель': 4,\n",
    "        'май': 5, 'июнь': 6, 'июль': 7, 'август': 8,\n",
    "        'сентябрь': 9, 'октябрь': 10, 'ноябрь': 11, 'декабрь': 12\n",
    "    }\n",
    "    month_str, year_str = date_str.lower().split()\n",
    "    month = months.get(month_str)\n",
    "    year = int(year_str)\n",
    "\n",
    "    return datetime(year, month, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результатом пасинга с каждого сайта будет аля jsonlines формата:\n",
    "\n",
    "## Формат данных \n",
    "\n",
    "Результат представляет собой массив словарей, где каждый словарь содержит информацию об одном аналитическом материале. Каждый словарь имеет следующие ключи:\n",
    "\n",
    "### Ключи словаря:\n",
    "1. **`link`** (`str`):\n",
    "   - Ссылка на страницу с аналитическим материалом. Может быть как полная, так и относительная\n",
    "   - Пример: `\"/news/12345\"`.\n",
    "\n",
    "2. **`date`** (`str`):\n",
    "   - Дата публикации материала.\n",
    "   - Пример: `\"2023-10-15\"`.\n",
    "\n",
    "3. **`name`** (`str`):\n",
    "   - Название материала на русском языке.\n",
    "   - Пример: `\"Анализ рынка ИТ в 2023 году\"`.\n",
    "\n",
    "4. **`name_en`** (`str`):\n",
    "   - Название материала на английском языке, извлеченное из ссылки.\n",
    "   - Пример: `\"it-market-analysis-2023\"`.\n",
    "\n",
    "5. **`tags`** (`list` of `str`):\n",
    "   - Список тегов, связанных с материалом.\n",
    "   - Пример: `[\"ИТ\", \"аналитика\", \"2023\"]`.\n",
    "\n",
    "6. **`pdf_links`** (`list` of `str`):\n",
    "   - Список ссылок на PDF-файлы, связанные с материалом.\n",
    "   - Пример: `[\"https://b1.ru/local/assets/surveys/it-market-analysis-2023.pdf\"]`.\n",
    "\n",
    "7. **`text`** (`str`):\n",
    "   - Основной текст материала из статьи на сайте (в pdf больше информации), очищенный от лишних элементов (контакты, реклама, скрипты и т.д.).\n",
    "   - Пример: `\"В 2023 году рынок ИТ показал рост на 14%...\"`.\n",
    "\n",
    "8. **`pdfs`** (`list` of `dict`):\n",
    "   - Список словарей, содержащих информацию о скачанных PDF-файлах. Каждый словарь содержит:\n",
    "     - **`name`** (`str`): Название PDF-файла.\n",
    "       - Пример: `\"it-market-analysis-2023.pdf\"`.\n",
    "     - **`content`** (`bytes`): Бинарное содержимое PDF-файла.\n",
    "\n",
    "---\n",
    "\n",
    "### Пример элемента списка `analytics`:\n",
    "```json\n",
    "{\n",
    "    \"link\": \"/news/12345\",\n",
    "    \"date\": \"2023-10-15\",\n",
    "    \"name\": \"Анализ рынка ИТ в 2023 году\",\n",
    "    \"name_en\": \"it-market-analysis-2023\",\n",
    "    \"tags\": [\"ИТ\", \"аналитика\", \"2023\"],\n",
    "    \"pdf_links\": [\n",
    "        \"https://b1.ru/local/assets/surveys/it-market-analysis-2023.pdf\"\n",
    "    ],\n",
    "    \"text\": \"В 2023 году рынок ИТ показал рост на 14%...\",\n",
    "    \"pdfs\": [\n",
    "        {\n",
    "            \"name\": \"it-market-analysis-2023.pdf\",\n",
    "            \"content\": \"binary_pdf_content_here\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b1-surveys\n",
    "Парсим данные со страницы https://b1.ru/b1-surveys/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://b1.ru/'\n",
    "SURVEYS_URL = URL + 'b1-surveys/'\n",
    "\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кнопка 'Показать еще' больше не найдена или недоступна.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "def get_chrome_options():\n",
    "    # Настройки для подключения к удаленному Selenium\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # Запуск в фоновом режиме\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--window-size=2560,1440')  # Принудительно устанавливает размер окна\n",
    "    options.add_argument('--start-maximized')\n",
    "    options.add_argument('--start-fullscreen')\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument('--disable-gpu')  # Отключает аппаратное ускорение\n",
    "    options.add_argument('--disable-infobars')  # Отключает информационные баннеры Chrome\n",
    "    options.add_argument('--disable-features=TranslateUI')  # Отключает встроенный переводчик Chrome\n",
    "    options.add_argument('--disable-popup-blocking')  # Отключает блокировку всплывающих окон\n",
    "    options.add_argument('--disable-extensions')  # Отключает все расширения\n",
    "    options.add_argument('--disable-notifications')  # Отключает уведомления\n",
    "    options.add_argument('--disable-background-networking')  # Запрещает Chrome использовать сеть в фоне\n",
    "    options.add_argument('--disable-sync')  # Отключает синхронизацию браузера\n",
    "    options.add_argument('--disable-logging')  # Отключает логи браузера\n",
    "    options.add_argument('--remote-debugging-port=9222')  # Включает отладку (для анализа поведения)\n",
    "    options.add_argument('--force-device-scale-factor=1')  # Отключает автоадаптацию экрана\n",
    "    options.add_argument('--disable-gesture-requirement-for-media-playback')  # Отключает автопроигрывание медиа\n",
    "\n",
    "    # Прячем Selenium (чтобы сайт не распознавал бота)\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "    # Устанавливаем user-agent браузера (чтобы эмулировать реальный Chrome)\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\")\n",
    "    return options\n",
    "\n",
    "\n",
    "driver = webdriver.Remote(command_executor='http://chrome.dev.pp.ru:4444/wd/hub', options=get_chrome_options())\n",
    "\n",
    "try:\n",
    "    driver.get(SURVEYS_URL)\n",
    "    wait = WebDriverWait(driver, 5)\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            show_more_button = wait.until(EC.element_to_be_clickable((By.ID, \"show-more\")))\n",
    "            driver.execute_script(\"arguments[0].click();\", show_more_button)\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"Кнопка 'Показать еще' больше не найдена или недоступна.\")\n",
    "            break\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при ожидании элемента: {e}\")\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics = []\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "for block in soup.find_all('div', class_=re.compile(r\"^news-block__gridItem triangleWrap--hover\")):\n",
    "    if block:\n",
    "        link = block['src-href']\n",
    "        if not 'services' in link:\n",
    "            analytics_block = {}\n",
    "            analytics_block['link'] = link\n",
    "            analytics_block['date'] = datetime.strptime(block.find('p', class_='news-block__date').text, '%d.%m.%Y')\n",
    "            analytics_block['name'] = block.find('h2', class_='news-block__title').text\n",
    "            analytics_block['name_en'] = link.strip().strip('/').split('/')[-1]\n",
    "            \n",
    "            time.sleep(1)\n",
    "            analytics_soup = get_soup(URL + link)\n",
    "            tags_block = analytics_soup.find('div', class_=re.compile(r\"tezis-list-block__tags\"))\n",
    "            if tags_block:\n",
    "                analytics_block['tags'] = [a.text for a in tags_block.find_all('a', class_=re.compile(r\"tag\"))]\n",
    "            else:\n",
    "                analytics_block['tags'] = []\n",
    "            \n",
    "            analytics_block['pdf_links'] = [f\"{URL}{a['href']}\" for a in analytics_soup.find_all('a', href=True) if a['href'].endswith('.pdf')]\n",
    "            analytics_block['pdf_links'].append(f'{URL}local/assets/surveys/{analytics_block[\"name_en\"]}.pdf')\n",
    "\n",
    "            main_text = analytics_soup.find('main')\n",
    "            for el in main_text.find_all('section', attrs={\"data-block-name\": \"Контакты\"}):\n",
    "                el.extract()\n",
    "            for el in main_text.find_all('section', attrs={\"data-block-name\": \"СКАЧАТЬ ПОЛНУЮ ВЕРСИЮ\"}):\n",
    "                el.extract()\n",
    "            for el in main_text.find_all('div', attrs={\"class\": \"page-cover\"}):\n",
    "                el.extract() \n",
    "            for el in main_text.find_all('script'):\n",
    "                el.extract()\n",
    "            for el in main_text.find_all('style'):\n",
    "                el.extract()\n",
    "            if tags_block:\n",
    "                for el in tags_block:\n",
    "                    el.extract()\n",
    "            analytics_block['text'] = convert_to_text(main_text)\n",
    "            \n",
    "            pdfs_content = []\n",
    "            pdfs_names = []\n",
    "            for url in analytics_block['pdf_links']:\n",
    "                for i in range(10):\n",
    "                    try:\n",
    "                        response = session.get(url, stream=True)\n",
    "                        if response.status_code == 200:\n",
    "                            pdf_name = url.strip().strip('/').split('/')[-1]\n",
    "                            pdf_content = response.content\n",
    "                            if pdf_name not in pdfs_names:\n",
    "                                pdfs_names.append(pdf_name)\n",
    "                                pdfs_content.append({\"name\": pdf_name, \"content\": pdf_content})\n",
    "                            break\n",
    "                    except:\n",
    "                        time.sleep(3)\n",
    "                    \n",
    "            analytics_block['pdfs'] = pdfs_content\n",
    "            analytics.append(analytics_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(analytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('b1_analytics.pkl', 'wb') as f:\n",
    "    pickle.dump(analytics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kamaflow\n",
    "Парсим данные с сайта https://kamaflow.com/ru/blog/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://strategy.ru'\n",
    "\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "url = URL + \"/api/v1/research/research\"\n",
    "params = {\"limit\": 10, \"offset\": 0}\n",
    "response = session.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(f\"Ошибка: {response.status_code}\")\n",
    "    print(response.text)\n",
    "\n",
    "researches_links = []\n",
    "k = 10\n",
    "for i in range(0, data['count'] + 1, k):\n",
    "    params = {\"limit\": k, \"offset\": i}\n",
    "    response = session.get(url, params=params)\n",
    "    data = response.json()\n",
    "    researches_links += data['researches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [01:27<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "researches = []\n",
    "for research_block in tqdm(researches_links):\n",
    "    link = URL + research_block['absolute_url']\n",
    "    date = convert_date(research_block['published_at'])\n",
    "    tags = [ind['title'] for ind in research_block['industries']]\n",
    "    name = research_block['title']\n",
    "    text_preview = research_block['preview']\n",
    "    \n",
    "    info = {'link': link, 'date': date, 'name': name, 'text_preview': text_preview, 'tags': tags}\n",
    "    time.sleep(1)\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            soup = get_soup(info['link'])\n",
    "            publication_div = soup.find('div', class_='publication__text tiny')\n",
    "            pdf_links = []\n",
    "            for btn in publication_div.find_all('a', class_='btn-download'):\n",
    "                pdf_links.append(URL + btn.extract()['href'].replace('../', '/'))\n",
    "            for el in publication_div.find_all('script'):\n",
    "                    el.extract()\n",
    "            for el in publication_div.find_all('style'):\n",
    "                el.extract()\n",
    "            info['text'] = convert_to_text(publication_div)\n",
    "            info['pdf_links'] = list(set(pdf_links))\n",
    "            pdfs_content = []\n",
    "            for pdf_link in info['pdf_links']:\n",
    "                response = session.get(pdf_link, stream=True)\n",
    "                if response.status_code == 200:\n",
    "                    pdf_name = url.strip().strip('/').split('/')[-1]\n",
    "                    pdf_content = response.content\n",
    "                    pdfs_content.append({\"name\": info['name'], \"content\": pdf_content})\n",
    "            info['pdfs'] = pdfs_content\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "    \n",
    "    researches.append(info)\n",
    "with open('kamaflow_researches.pkl', 'wb') as f:\n",
    "    pickle.dump(researches, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Внутренние источники"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные из аналитического хаба. Данные из  pdf  были получены с помощью PyPDFLoader.  \n",
    "Дозаполним данные, чтобы структура данных была одинаковая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('industry_hub_analytics.pkl', 'rb') as f:\n",
    "    industry_hub_analytics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [02:22<00:00,  8.90s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=['name', \"text\"],\n",
    "    template=(\n",
    "        \"Определи или извлеки наиболее вероятную дату создания документа. Докуменит представляет из себя отраслевая аналитика или отчёт на определённый период времени.\"\n",
    "        \"Если точный день неизвестен, укажи месяц и год. Если есть несколько дат, выбери наиболее вероятную. Учти, что даты на русском поэтому, скорее всего, если дата начианется с года, то потом идёт месяц, либо сначала идёт день, а потом месяц.\"\n",
    "        \"Выведи только дату в формате YYYY-MM-DD.\"\n",
    "        \"Наименование самого документа: :\\n\\n{name}\\n\\n\"\n",
    "        \"Текст документа (извлёчен из pdf), :\\n\\n{text}\\n\\n\"\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "industry_hub_analytics_update = []\n",
    "\n",
    "for hub in tqdm(industry_hub_analytics):\n",
    "    for pdf_block in hub['pdfs']:\n",
    "        response = llm.invoke(prompt.format(name=pdf_block['name'], text=pdf_block['full_text'])[:1_500])\n",
    "    \n",
    "        data = {\n",
    "            \"link\": hub['link'],\n",
    "            \"date\": response.content,\n",
    "            \"name\": pdf_block['name'],\n",
    "            \"tags\": hub['tags'],\n",
    "            \"text\": \"\",\n",
    "            \"pdfs\": [pdf_block]\n",
    "        }\n",
    "        industry_hub_analytics_update.append(data)\n",
    "        \n",
    "with open('industry_hub_analytics_update.pkl', 'wb') as f:\n",
    "    pickle.dump(industry_hub_analytics_update, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
