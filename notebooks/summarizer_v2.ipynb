{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Иcточники данных\n",
    "\n",
    "- Внешние источники\n",
    "  - b1: `b1_analytics.pkl`\n",
    "  - kamaflow: `kamaflow_researches.pkl`\n",
    "- Внутренние источники\n",
    "  - Отраслевой хаб: данные выложены в sberdisk\n",
    "\n",
    "## Формат данных \n",
    "\n",
    "Результат представляет собой массив словарей, где каждый словарь содержит информацию об одном аналитическом материале. Каждый словарь имеет следующие ключи:\n",
    "\n",
    "### Пример элемента списка `analytics`:\n",
    "```json\n",
    "{\n",
    "    \"link\": \"/news/12345\",\n",
    "    \"date\": \"2023-10-15\",\n",
    "    \"name\": \"Анализ рынка ИТ в 2023 году\",\n",
    "    \"name_en\": \"it-market-analysis-2023\",\n",
    "    \"tags\": [\"ИТ\", \"аналитика\", \"2023\"],\n",
    "    \"pdf_links\": [\n",
    "        \"https://b1.ru/local/assets/surveys/it-market-analysis-2023.pdf\"\n",
    "    ],\n",
    "    \"text\": \"В 2023 году рынок ИТ показал рост на 14%...\",\n",
    "    \"pdfs\": [\n",
    "        {\n",
    "            \"name\": \"it-market-analysis-2023.pdf\",\n",
    "            \"content\": \"binary_pdf_content_here\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "Мы хотим получить такой, при этом часть полей можент быть пустой:\n",
    "```json\n",
    "{\n",
    "    \"link\": \"/news/12345\",\n",
    "    \"date\": \"2023-10-15\",\n",
    "    \"name\": \"Анализ рынка ИТ в 2023 году\",\n",
    "    \"name_en\": \"it-market-analysis-2023\",\n",
    "    \"tags\": [\"ИТ\", \"аналитика\", \"2023\"],\n",
    "    \"sber_tags\": [\"Теги\", \"из\", \"сбера\", \"отрасль\", \"из\", \"kksb_clinet_profile\"],\n",
    "    \"pdf_links\": [\n",
    "        \"https://b1.ru/local/assets/surveys/it-market-analysis-2023.pdf\"\n",
    "    ],\n",
    "    \"text\": \"В 2023 году рынок ИТ показал рост на 14%...\",\n",
    "    \"summary\": \"Саммари на основе pdfs\",\n",
    "    \"pdfs\": [\n",
    "        {\n",
    "            \"name\": \"it-market-analysis-2023.pdf\",\n",
    "            \"content\": \"binary_pdf_content_here\",\n",
    "            \"full_text\": \"Полный текст и pdf, то что получилось вытащить из content\",\n",
    "            \"summary\": \"Саммари из full_text\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Для дальнейнего анализа нужно только следующие поля: date, sber_tags, summary, pdfs['full_text'], pdfs['summary']:\n",
    "```json\n",
    "{\n",
    "    \"date\": \"2023-10-15\",\n",
    "    \"sber_tags\": [\"Теги\", \"из\", \"сбера\", \"отрасль\", \"из\", \"kksb_clinet_profile\"],\n",
    "    \"summary\": \"Саммари\",\n",
    "    \"pdfs\": [\n",
    "        {\n",
    "            \"full_text\": \"Полный текст и pdf, то что получилось вытащить из content\",\n",
    "            \"summary\": \"Саммари из full_text\",\n",
    "        },\n",
    "        {\n",
    "            \"full_text\": \"Полный текст и pdf, то что получилось вытащить из content\",\n",
    "            \"summary\": \"Саммари из full_text\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('b1_analytics.pkl', 'rb') as f:\n",
    "    b1_analytics = pickle.load(f)\n",
    "\n",
    "with open('kamaflow_researches.pkl', 'rb') as f:\n",
    "    kamaflow_researches = pickle.load(f)\n",
    "\n",
    "# with open('/Users/22926900/Desktop/consult_plan_v3/industry_analytics_parser/notebooks/industry_hub_analytics_update.pkl', 'rb') as f:\n",
    "#     industry_hub_analytics = pickle.load(f)\n",
    "\n",
    "docs = b1_analytics + kamaflow_researches #+ industry_hub_analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_json_blocks(text):\n",
    "    \"\"\"\n",
    "    Извлекает содержимое всех блоков ```json ... ``` из текста.\n",
    "    \n",
    "    Аргументы:\n",
    "        text (str): Исходный текст, содержащий блоки с JSON.\n",
    "        \n",
    "    Возвращает:\n",
    "        str: Содержимое всех JSON-блоков, объединенное через пробел.\n",
    "             Если блоков не найдено, возвращает пустую строку.\n",
    "    \"\"\"\n",
    "    pattern = r'```json(.*?)```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    \n",
    "    # Объединяем все найденные блоки через пробел и убираем лишние пробелы\n",
    "    result = ' '.join(match.strip() for match in matches)\n",
    "    return result\n",
    "\n",
    "def fix_json_commas(json_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Fixes common JSON formatting issues related to commas.\n",
    "    \n",
    "    Args:\n",
    "        json_str (str): Input JSON string\n",
    "        \n",
    "    Returns:\n",
    "        str: Properly formatted JSON string\n",
    "    \"\"\"\n",
    "    # Remove trailing commas before closing braces/brackets\n",
    "    json_str = json_str.replace(\",}\", \"}\")\n",
    "    json_str = json_str.replace(\",\\n}\", \"}\")\n",
    "    json_str = json_str.replace(\",\\n  }\", \"}\")\n",
    "    json_str = json_str.replace(\",\\n    }\", \"}\")\n",
    "    json_str = json_str.replace(\",]\", \"]\")\n",
    "    json_str = json_str.replace(\",\\n]\", \"]\")\n",
    "    json_str = json_str.replace(\",\\n  ]\", \"]\")\n",
    "    json_str = json_str.replace(\",\\n    ]\", \"]\")\n",
    "    \n",
    "    # Add missing commas between objects\n",
    "    json_str = json_str.replace(\"}\\n  {\", \"},\\n  {\")\n",
    "    json_str = json_str.replace(\"}\\n    {\", \"},\\n    {\")\n",
    "    \n",
    "    return json_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавляем саммари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1-mini\")\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Ты стратегический консультант. Ты изучаешь и обрабатываешь отраслевые данные, новости, исследжования.\n",
    "Твоя задача - провести анализ и сформировать структурированный отчёт о 1-3 ключевых трендах в формате JSON.\n",
    "\n",
    "## JSON Schema\n",
    "```json\n",
    "{{\n",
    "  \"type\": \"array\",\n",
    "  \"items\": {{\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\"тренд\", \"факт\", \"анализ\"],\n",
    "    \"properties\": {{\n",
    "      \"тренд\": {{\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"Краткая суть тренда в виде законченного утверждения. Например: 'рост спроса на экологичные товары'\"\n",
    "        }},\n",
    "      \"факт\": {{\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"Конкретные цифры, данные или утверждения из текста, подтверждающие тренд. Например: '73% потребителей готовы платить больше за экологичную упаковку'\"\n",
    "        }}, \n",
    "      \"анализ\": {{\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"3-7 связанных предложений, раскрывающих: причины тренда, его последствия, влияние на рынок/бизнес, прогноз развития\"\n",
    "        }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## Правила валидации JSON:\n",
    "1. Используй ТОЛЬКО двойные кавычки (\") для строк\n",
    "2. Ставь запятую после КАЖДОГО элемента, кроме последнего\n",
    "3. НЕ СТАВЬ запятую после последнего элемента в объекте или массиве\n",
    "4. Все скобки {{}} и [] должны быть закрыты правильно\n",
    "5. Экранируй кавычки внутри строк как \\\\\"\n",
    "6. Не используй комментарии в JSON\n",
    "7. При отсутствии данных возвращай пустой массив []\n",
    "8. Экранируй спецсимволы согласно спецификации JSON:\n",
    "   - \\\\\" для кавычек\n",
    "   - \\\\\\\\ для обратной косой черты\n",
    "   - \\\\n для переноса строки\n",
    "   - \\\\t для табуляции\n",
    "8. JSON должен быть валидным для стандартного парсера\n",
    "\n",
    "## Формат вывода:\n",
    "```json\n",
    "[\n",
    "  {{\n",
    "    \"тренд\": \"суть тренда из данных\",\n",
    "    \"факт\": \"подтверждающие факты\",\n",
    "    \"анализ\": \"анализ тренда в 5-7 предложений\"\n",
    "  }}\n",
    "]\n",
    "```\n",
    "\n",
    "## Пример правильного JSON без лишних запятых:\n",
    "```json\n",
    "[\n",
    "  {{\n",
    "    \"тренд\": \"пример тренда\",\n",
    "    \"факт\": \"пример факта\",\n",
    "    \"анализ\": \"пример анализа\"\n",
    "  }},\n",
    "  {{\n",
    "    \"тренд\": \"второй тренд\",\n",
    "    \"факт\": \"второй факт\",\n",
    "    \"анализ\": \"второй анализ\"\n",
    "  }}\n",
    "]\n",
    "```\n",
    "\n",
    "## Требования к содержанию:\n",
    "1. Извлекай только явные тренды из текста\n",
    "2. Подтверждай трендыконкретными фактами\n",
    "3. Анализируй тренд в 5-7 предложений\n",
    "4. Пиши только на русском языке\n",
    "5. Возвращай только валидный JSON\n",
    "6. Все кавычки внутри строк обязательно экранируй: \\\\\" — иначе JSON будет невалидным\n",
    "\n",
    "\n",
    "## Входные данные:\n",
    "- Название: {name}\n",
    "- Теги: {tags} \n",
    "- Краткое содержание: {additional_text}\n",
    "- Текст документа:\n",
    "```\n",
    "{text_document}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Ты — стратегический консультант. На основе отраслевых данных, новостей и исследований тебе нужно выявить 1–3 ключевых тренда, подтверждённых фактами, и представить их в виде валидного JSON по заданной схеме.\n",
    "\n",
    "## Твоя задача:\n",
    "1. Найди до трёх трендов из текста.\n",
    "2. Каждый тренд должен быть явно выражен, подтверждён конкретным фактом и проанализирован.\n",
    "3. Анализ включает причины, последствия, влияние на рынок и прогноз развития (3–7 предложений).\n",
    "4. Пиши только на русском языке.\n",
    "\n",
    "## Формат вывода (JSON):\n",
    "```json\n",
    "[\n",
    "  {{\n",
    "    \"тренд\": \"Краткое утверждение, отражающее суть тренда (например: \\\\\"рост спроса на экологичные товары\\\\\")\",\n",
    "    \"факт\": \"Цифры, статистика или цитата из текста, подтверждающая тренд (например: \\\\\"73% потребителей готовы платить больше за экологичную упаковку\\\\\")\",\n",
    "    \"анализ\": \"Связный аналитический текст из 3–7 предложений, раскрывающий причины, влияние, последствия и прогноз тренда\"\n",
    "  }}\n",
    "]\n",
    "```\n",
    "\n",
    "## Требования к JSON:\n",
    "- Используй только двойные кавычки (\")\n",
    "- После каждого поля ставь запятую, кроме последнего\n",
    "- Не используй комментарии\n",
    "- Все кавычки внутри строк экранируй: \\\\\" \n",
    "- Экранируй спецсимволы: \\\\\\\\ — обратная косая, \\\\n — перенос строки, \\\\t — табуляция\n",
    "- При отсутствии трендов верни: `[]`\n",
    "- Все кавычки внутри строк обязательно экранируй: \\\\\" — иначе JSON будет невалидным\n",
    "- JSON должен быть полностью валидным\n",
    "\n",
    "## Входные данные:\n",
    "- Название: {name}\n",
    "- Теги: {tags} \n",
    "- Краткое содержание: {additional_text}\n",
    "- Текст документа:\n",
    "```\n",
    "{text_document}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "messages = ChatPromptTemplate.from_messages([\n",
    "    ('system', prompt),\n",
    "    ('human', \"Выведи итоговый JSON\"),\n",
    "])\n",
    "\n",
    "\n",
    "chain = (\n",
    "    messages\n",
    "    | llm \n",
    "    | (lambda x: fix_json_commas(x.content))\n",
    "    | JsonOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 95/99 [18:58<00:55, 13.94s/it]invalid pdf header: b'RIFF\\x8c'\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing PDF Супертренд на здоровое питание — ключевая тенденция на рынке хлебобулочных и кондитерских изделий в России: Stream has ended unexpectedly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [19:11<00:00, 11.63s/it]\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for doc in tqdm(docs):\n",
    "    name = doc['name']\n",
    "    tags = doc['tags']\n",
    "    additional_text = doc['text']\n",
    "    \n",
    "    text_documents = []\n",
    "    \n",
    "    for pdf in doc['pdfs']:\n",
    "        pdf_name = pdf['name']\n",
    "\n",
    "        pdf_content = pdf.get('content', None)\n",
    "        full_text = pdf.get('full_text', None)\n",
    "        if full_text is None or len(full_text) > 0:\n",
    "            counter += 1\n",
    "        if full_text is None and pdf_content is not None:\n",
    "            try:\n",
    "                with tempfile.NamedTemporaryFile() as temp_file:\n",
    "                    temp_file.write(pdf_content)\n",
    "                    temp_file_path = temp_file.name\n",
    "                    loader = PyPDFLoader(temp_file_path)\n",
    "                    pdf_data = loader.load()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing PDF {pdf_name}: {e}\")\n",
    "                pdf_data = None\n",
    "            if pdf_data:\n",
    "                full_text = \"\\n\".join([doc.page_content for doc in pdf_data])\n",
    "                pdf['full_text'] = full_text\n",
    "        \n",
    "        if full_text:\n",
    "            if additional_text is None or len(additional_text) == 0:\n",
    "                additional_text = 'нет краткого описание'\n",
    "            text_document = f'**Документ с наименованием {pdf_name}, его содержимое:** {full_text}'\n",
    "            text_documents.append(text_document)\n",
    "    if len(text_documents) == 0:\n",
    "        doc['summary'] = ''\n",
    "    else:\n",
    "        text_document = \"\\n\".join(text_documents)\n",
    "        doc['summary'] = chain.invoke(\n",
    "                {\n",
    "                    'name': name,\n",
    "                    'tags': \", \".join(tags), \n",
    "                    'additional_text': additional_text,\n",
    "                    'text_document': text_document[:50_000 - 3] + '...',\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавляем теги\n",
    "Теги из kksb_client_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [02:00<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Данные\n",
    "data = {\n",
    "    \"date\": \"2023-10-15\",\n",
    "    \"summary\": \"Саммари\",\n",
    "    \"pdfs\": [\n",
    "        {\n",
    "            \"full_text\": \"Полный текст и pdf, то что получилось вытащить из content\",\n",
    "            \"summary\": \"Саммари из full_text\",\n",
    "        },\n",
    "        {\n",
    "            \"full_text\": \"Полный текст и pdf, то что получилось вытащить из content\",\n",
    "            \"summary\": \"Саммари из full_text\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# Возможные теги\n",
    "tags_list = [\n",
    "    'Энергетика', 'Лизинг', 'Нефтегазовая промышленность', 'Пищевая промышленность',\n",
    "    'Финансы', 'Органы государственного и муниципального управления', 'Материалы', 'Промышленность',\n",
    "    'Телекоммуникации', 'Телекоммуникации и медиа', 'ИТ-технологии', 'ЖКХ', 'Услуги',\n",
    "    'Электроэнергетика и ЖКХ', 'Строительные подрядчики', 'Сельское хозяйство', 'Товары первой необходимости',\n",
    "    'Энергоносители', 'Товары выборочного спроса', 'Операции с недвижимым имуществом', 'Государство',\n",
    "    'Недвижимость', 'Фармацевтика и здравоохранение', 'Химическая промышленность', 'Розничная торговля товарами первой необходимости',\n",
    "    'Машиностроение', 'Металлургическая и горнодобывающая промышленность', 'Производство строительных материалов',\n",
    "    'Розничная торговля товарами выборочного спроса', 'Финансовая деятельность', 'Легкая промышленность',\n",
    "    'Транспорт и логистика', 'Производство потребительских товаров', 'Лесная и деревообрабатывающая и целлюлозно-бумажная промышленность', 'Туризм'\n",
    "]\n",
    "tags_list = [tag.lower() for tag in tags_list]\n",
    "\n",
    "for data in tqdm(docs):\n",
    "    text_sources = [\n",
    "        'Отраслевые теги из документа: ',\n",
    "        ', '.join([x for x in data[\"tags\"] if x != 'Прочее']), \n",
    "        '\\n Текст трендов: ', str(data[\"summary\"]), \n",
    "    ]\n",
    "    context_text = \"\\n\".join(filter(None, text_sources))[:40_000]\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\", \"tags\"],\n",
    "        template=(\n",
    "            \"\"\"Определи, какие из следующих тегов отрасли наиболее подходят для данного текста с отраслевыми трендами:\n",
    "            ## Теги отрасли: \n",
    "            {tags}. \n",
    "            ## Тренды:\n",
    "            {text}\n",
    "            ## Формат ответа:\n",
    "            Отвечай только списком релевантных трендам тегов через запятую, без пояснений. Пиши только те теги, которые релевантны трендам и есть в списке теги отрасли:{tags}. Не выдумывай другие теги и не изменя  формулировку этих тегов. Не пиши ничего кроме тегов. Если релевантных тегов нет не пиши ничего.\"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(prompt.format(text=context_text, tags=\", \".join(tags_list))).content.strip()\n",
    "    data[\"sber_tags\"] = [tag.strip() for tag in response.split(\",\") if tag.strip().lower() in tags_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сохраняемся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs.pkl', 'wb') as f:\n",
    "    pickle.dump(docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_str):\n",
    "    # Пробуем разные форматы дат\n",
    "    for fmt in ('%d.%m.%Y', '%Y-%m-%d', '%d-%m-%Y', '%Y.%m.%d', '%y-%m-%d', '%d/%m/%Y'):\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt, errors='raise')\n",
    "        except:\n",
    "            continue\n",
    "    return pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "new_docs = []\n",
    "for row_counter, doc in enumerate(docs):\n",
    "    new_doc = {\n",
    "        \"date\": doc['date'],\n",
    "        \"sber_tags\": ', '.join(list(set(doc['sber_tags']))),\n",
    "        \"summary\": doc['summary'],\n",
    "        'row_id': row_counter,\n",
    "    }\n",
    "    if isinstance(new_doc['date'], datetime):\n",
    "        new_doc['date'] = new_doc['date'].strftime('%d.%m.%Y')\n",
    "    # pdfs = []\n",
    "    # for pdf in doc['pdfs']:\n",
    "    #     pdfs.append({\n",
    "    #         \"full_text\": pdf.get('full_text', ''),\n",
    "    #         # \"summary\": pdf['summary'],\n",
    "    #     })\n",
    "    # new_doc['pdfs'] = pdfs\n",
    "    new_docs.append(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(new_docs).rename(columns={'date': 'date_sum'})\n",
    "df['date_sum'] = df['date_sum'].apply(parse_date).dt.strftime('%d.%m.%Y')\n",
    "df['sber_tags'] = df['sber_tags'].str.replace('\\s*,\\s*', ',', regex=True).str.split(',')\n",
    "df = df.explode('sber_tags')\n",
    "df['sber_tags'] = df['sber_tags'].str.strip()\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.dropna(subset=['date_sum'], how='any')\n",
    "df.to_excel('trends_with_dates.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
